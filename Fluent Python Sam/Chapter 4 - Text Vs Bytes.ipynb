{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding and Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "b'caf\\xc3\\xa9'\n",
      "5\n",
      "<class 'bytes'>\n",
      "café\n"
     ]
    }
   ],
   "source": [
    "s = 'café' \n",
    "print(len(s)) # The str 'café' has four Unicode characters\n",
    "b = s.encode('utf8') # Encode str to bytes using UTF-8 encoding\n",
    "print(b) # bytes literals start with a b prefix i.e. b'caf\\xc3\\xa9'\n",
    "print(len(b)) # bytes b has five bytes (the code point for “é” is encoded as two bytes in UTF-8)\n",
    "print(type(b))\n",
    "print(b.decode('utf8')) # Decode bytes to str using UTF-8 encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Byte Essentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'caf\\xc3\\xa9'\n",
      "99\n",
      "b'c'\n",
      "bytearray(b'caf\\xc3\\xa9')\n",
      "bytearray(b'\\xa9')\n"
     ]
    }
   ],
   "source": [
    "cafe = bytes('café', encoding='utf_8') # bytes can be built from a str, given an encoding\n",
    "print(cafe) # the first three bytes b'caf' are in the printable ASCII range (0-255), the last two are not\n",
    "\n",
    "print(cafe[0]) # Each item is an integer in range(256)\n",
    "\n",
    "print(cafe[:1]) # Slices of bytes are also bytes—even slices of a single byte\n",
    "cafe_arr = bytearray(cafe) #\n",
    "print(cafe_arr) #There is no literal syntax for bytearray: they are shown as bytearray() with a bytes literal as argument\n",
    "\n",
    "print(cafe_arr[-1:]) # A slice of bytearray is also a bytearray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'1K\\xce\\xa9'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Binary sequences have a class method that str doesn’t have, called fromhex, which\n",
    "# builds a binary sequence by parsing pairs of hex digits optionally separated by spaces:\n",
    "bytes.fromhex('31 4B CE A9')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\xfe\\xff\\xff\\xff\\x00\\x00\\x01\\x00\\x02\\x00'\n"
     ]
    }
   ],
   "source": [
    "# Initializing bytes from raw array data\n",
    "\n",
    "import array\n",
    "numbers = array.array('h', [-2, -1, 0, 1, 2]) # Typecode 'h' creates an array of short integers (16 bits)\n",
    "octets = bytes(numbers) # octets holds a copy of the bytes that make up numbers\n",
    "print(octets) # These are the 10 bytes that represent the five short integers\n",
    "\n",
    "# Creating a bytes or bytearray object from any buffer-like source will always copy\n",
    "# the bytes. In contrast, memoryview objects let you share memory between binary data\n",
    "# structures. To extract structured information from binary sequences, the struct\n",
    "# module is invaluable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structs and Memory Views\n",
    "\n",
    "# The struct module provides functions to parse packed bytes into a tuple of fields of\n",
    "# different types and to perform the opposite conversion, from a tuple into packed\n",
    "# bytes. struct is used with bytes, bytearray, and memoryview objects\n",
    "\n",
    "# the memoryview class does not let you\n",
    "# create or store byte sequences, but provides shared memory access to slices of data\n",
    "# from other binary sequences, packed arrays, and buffers such as Python Imaging\n",
    "# Library (PIL) images,2 without copying the bytes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Example 4-4. Using memoryview and struct to inspect a GIF image header\n",
    "\n",
    "import struct\n",
    "\n",
    "fmt = '<3s3sHH' # struct format: < little-endian; 3s3s two sequences of 3 bytes; HH two 16-bit integers\n",
    "\n",
    "with open('filter.gif', 'rb') as fp:\n",
    "... img = memoryview(fp.read()) # Create memoryview from file contents in memory\n",
    "...\n",
    "\n",
    "header = img[:10] # then another memoryview by slicing the first one; no bytes are copied here\n",
    "bytes(header) # Convert to bytes for display only; 10 bytes are copied here\n",
    "b'GIF89a+\\x02\\xe6\\x00'\n",
    "\n",
    "struct.unpack(fmt, header) # Unpack memoryview into tuple of: type, version, width, and height\n",
    "(b'GIF', b'89a', 555, 230)\n",
    "\n",
    "del header # Delete references to release the memory associated with the memoryview instances\n",
    "del img\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Encoders/Decoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latin_1\tb'El Ni\\xf1o'\n",
      "utf_8\tb'El Ni\\xc3\\xb1o'\n",
      "utf_16\tb'\\xff\\xfeE\\x00l\\x00 \\x00N\\x00i\\x00\\xf1\\x00o\\x00'\n"
     ]
    }
   ],
   "source": [
    "# The Python distribution bundles more than 100 codecs (encoder/decoder) for text to\n",
    "# byte conversion and vice versa. Each codec has a name, like 'utf_8', and often\n",
    "# aliases, such as 'utf8', 'utf-8', and 'U8', which you can use as the encoding argument \n",
    "# in functions like open(), str.encode(), bytes.decode()\n",
    "\n",
    "\n",
    "# Example 4-5. The string “El Niño” encoded with three codecs producing very different byte sequences\n",
    "for codec in ['latin_1', 'utf_8', 'utf_16']:\n",
    "    print(codec, 'El Niño'.encode(codec), sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'S\\xc3\\xa3o Paulo'\n",
      "b'\\xff\\xfeS\\x00\\xe3\\x00o\\x00 \\x00P\\x00a\\x00u\\x00l\\x00o\\x00'\n",
      "b'S\\xe3o Paulo'\n",
      "'charmap' codec can't encode character '\\xe3' in position 1: character maps to <undefined>\n",
      "b'So Paulo'\n",
      "b'S?o Paulo'\n",
      "b'S&#227;o Paulo'\n"
     ]
    }
   ],
   "source": [
    "city = 'São Paulo'\n",
    "print(city.encode('utf_8')) # The 'utf_?' encodings handle any str\n",
    "\n",
    "print(city.encode('utf_16'))\n",
    "\n",
    "print(city.encode('iso8859_1')) # 'iso8859_1' also works for the 'São Paulo' str\n",
    "\n",
    "try: # 'cp437' can’t encode the 'ã' (“a” with tilde). The default error handler —'strict'—raises UnicodeEncodeError\n",
    "    print(city.encode('cp437'))\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "print(city.encode('cp437', errors='ignore')) # The error='ignore' handler silently skips characters that cannot be encoded; this is usually a very bad idea\n",
    "\n",
    "print(city.encode('cp437', errors='replace')) # When encoding, error='replace' substitutes unencodable characters with '?'; data is lost, but users will know something is amiss\n",
    "\n",
    "print(city.encode('cp437', errors='xmlcharrefreplace')) # 'xmlcharrefreplace' replaces unencodable characters with an XML entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to Discover the Encoding of a Byte Sequence\n",
    "\n",
    "# Chardet — The Universal Character Encoding Detector\n",
    "# works to identify one of 30 supported encodings. Chardet is a Python library that you\n",
    "# Understanding Encode/Decode Problems | 113can use in your programs, but also includes a command-line utility, chardetect.\n",
    "# Here is what it reports on the source file for this chapter:\n",
    "\n",
    "# $ chardetect 04-text-byte.asciidoc\n",
    "# 04-text-byte.asciidoc: utf-8 with confidence 0.99\n",
    "\n",
    "# Although binary sequences of encoded text usually don’t carry explicit hints of their\n",
    "# encoding, the UTF formats may prepend a byte order mark to the textual content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\xff\\xfeE\\x00l\\x00 \\x00N\\x00i\\x00\\xf1\\x00o\\x00'\n",
      "[255, 254, 69, 0, 108, 0, 32, 0, 78, 0, 105, 0, 241, 0, 111, 0]\n"
     ]
    }
   ],
   "source": [
    "# BOM: A Useful Gremlin\n",
    "\n",
    "u16 = 'El Niño'.encode('utf_16')\n",
    "print(u16)\n",
    "\n",
    "# The bytes are b'\\xff\\xfe'. That is a BOM—byte-order mark—denoting the “littleendian” \n",
    "# byte ordering of the Intel CPU where the encoding was performed\n",
    "\n",
    "# On a little-endian machine, for each code point the least significant byte comes first:\n",
    "# the letter 'E', code point U+0045 (decimal 69), is encoded in byte offsets 2 and 3 as 69 and 0:\n",
    "\n",
    "print(list(u16))\n",
    "\n",
    "# On a big-endian CPU, the encoding would be reversed; 'E' would be encoded as 0 and 69"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Text Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best practice for handling text is the “Unicode sandwich” (Figure 4-2).4 This\n",
    "means that bytes should be decoded to str as early as possible on input (e.g., when\n",
    "opening a file for reading). The “meat” of the sandwich is the business logic of your\n",
    "program, where text handling is done exclusively on str objects. You should never be\n",
    "encoding or decoding in the middle of other processing. On output, the str are enco‐\n",
    "ded to bytes as late as possible. Most web frameworks work like that, and we rarely\n",
    "touch bytes when using them. In Django, for example, your views should output\n",
    "Unicode str; Django itself takes care of encoding the response to bytes, using UTF-8\n",
    "by default\n",
    "\n",
    "Bytes -> str : Decode bytes on input  \n",
    "100% str : process using text only  \n",
    "str -> bytes : encode text back to bytes on output  \n",
    "\n",
    "Python 3 makes it easier to follow the advice of the Unicode sandwich, because the\n",
    "open built-in does the necessary decoding when reading and encoding when writing\n",
    "files in text mode, so all you get from my_file.read() and pass to\n",
    "my_file.write(text) are str objects.  \n",
    "\n",
    "Therefore, using text files is simple. But if you rely on default encodings you will get\n",
    "bitten.\n",
    "\n",
    "Example 4-9. A platform encoding issue (if you try this on your machine, you may or\n",
    "may not see the problem)\n",
    "```python\n",
    ">>> open('cafe.txt', 'w', encoding='utf_8').write('café')\n",
    "4\n",
    ">>> open('cafe.txt').read()\n",
    "'cafÃ©'\n",
    "```\n",
    "\n",
    "Moral of story: always specify encoding when reading/writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " locale.getpreferredencoding() -> 'UTF-8'\n",
      "                 type(my_file) -> <class '_io.TextIOWrapper'>\n",
      "           sys.stdout.isatty() -> False\n",
      "           sys.stdout.encoding -> 'UTF-8'\n",
      "            sys.stdin.isatty() -> False\n",
      "            sys.stdin.encoding -> 'UTF-8'\n",
      "           sys.stderr.isatty() -> False\n",
      "           sys.stderr.encoding -> 'UTF-8'\n",
      "      sys.getdefaultencoding() -> 'utf-8'\n",
      "   sys.getfilesystemencoding() -> 'utf-8'\n"
     ]
    }
   ],
   "source": [
    "# Encoding defaults: a madhouse\n",
    "\n",
    "# Several settings affect the encoding defaults for I/O in Python\n",
    "\n",
    "import sys, locale\n",
    "expressions = \"\"\"\n",
    "    locale.getpreferredencoding()\n",
    "    type(my_file)\n",
    "    sys.stdout.isatty()\n",
    "    sys.stdout.encoding\n",
    "    sys.stdin.isatty()\n",
    "    sys.stdin.encoding\n",
    "    sys.stderr.isatty()\n",
    "    sys.stderr.encoding\n",
    "    sys.getdefaultencoding()\n",
    "    sys.getfilesystemencoding()\n",
    "\"\"\"\n",
    "my_file = open('dummy', 'w')\n",
    "for expression in expressions.split():\n",
    "    value = eval(expression)\n",
    "    print(expression.rjust(30), '->', repr(value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above was run on ubuntu 18.04\n",
    "\n",
    "running the same code on windows results in: \n",
    "\n",
    "locale.getpreferredencoding() -> 'cp1252'  \n",
    "type(my_file) -> <class '_io.TextIOWrapper'>  \n",
    "my_file.encoding -> 'cp1252'  \n",
    "sys.stdout.isatty() -> True  \n",
    "sys.stdout.encoding -> 'cp850'  \n",
    "sys.stdin.isatty() -> True  \n",
    "sys.stdin.encoding -> 'cp850'  \n",
    "sys.stderr.isatty() -> True  \n",
    "sys.stderr.encoding -> 'cp850'  \n",
    "sys.getdefaultencoding() -> 'utf-8'  \n",
    "sys.getfilesystemencoding() -> 'mbcs  \n",
    "\n",
    "the most important encoding setting is that returned by **locale.getpreferredencoding()**: it is the default for opening text files and for sys.stdout/stdin/stderr when they are redirected to files.   \n",
    "\n",
    "However, the documentation reads (in part):\n",
    "```\n",
    "locale.getpreferredencoding(do_setlocale=True)\n",
    "Return the encoding used for text data, according to user preferences. User pref‐\n",
    "erences are expressed differently on different systems, and might not be available\n",
    "programmatically on some systems, so this function only returns a guess. […]\n",
    "```\n",
    "\n",
    "Therefore, the best advice about encoding defaults is: do not rely on them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing Unicode for Comparisons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "café café\n",
      "4 5\n",
      "False\n",
      "4 5\n",
      "4 4\n",
      "5 5\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# String comparisons are complicated by the fact that Unicode has combining characters: \n",
    "# diacritics and other marks that attach to the preceding character, appearing as one when printed.\n",
    "# For example, the word “café” may be composed in two ways, using four or five code\n",
    "# points, but the result looks exactly the same:\n",
    "\n",
    "s1 = 'café'\n",
    "s2 = 'cafe\\u0301'\n",
    "print(s1, s2)\n",
    "print(len(s1), len(s2))\n",
    "print(s1 == s2)\n",
    "\n",
    "# The code point U+0301 is the COMBINING ACUTE ACCENT. Using it after “e” renders\n",
    "# “é”. In the Unicode standard, sequences like 'é' and 'e\\u0301' are called “canonical\n",
    "# equivalents,” and applications are supposed to treat them as the same. But Python\n",
    "# sees two different sequences of code points, and considers them not equal\n",
    "\n",
    "# The solution is to use Unicode normalization, provided by the unicodedata.normalize \n",
    "# function. The first argument to that function is one of four strings: 'NFC', 'NFD', 'NFKC', and 'NFKD'.\n",
    "\n",
    "from unicodedata import normalize\n",
    "s1 = 'café' # composed \"e\" with acute accent\n",
    "s2 = 'cafe\\u0301' # decomposed \"e\" and acute accent\n",
    "print(len(s1), len(s2))\n",
    "print(len(normalize('NFC', s1)), len(normalize('NFC', s2)))\n",
    "print(len(normalize('NFD', s1)), len(normalize('NFD', s2)))\n",
    "\n",
    "# Normalization Form C (NFC) composes the code points to produce the shortest equivalent string\n",
    "print(normalize('NFC', s1) == normalize('NFC', s2))\n",
    "\n",
    "# NFD decomposes, expanding composed characters into base characters and separate combining characters\n",
    "print(normalize('NFD', s1) == normalize('NFD', s2))\n",
    "\n",
    "# Western keyboards usually generate composed characters, so text typed by users will\n",
    "# be in NFC by default. However, to be safe, it may be good to sanitize strings with\n",
    "# normalize('NFC', user_text) before saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“Herr Voß: • ½ cup of Œtker™ caffe latte • bowl of acai.”\n",
      "Ζεφυρος, Zefiro\n"
     ]
    }
   ],
   "source": [
    "# Example 4-14. Function to remove all combining marks (module sanitize.py)\n",
    "import unicodedata\n",
    "import string\n",
    "\n",
    "def shave_marks(txt):\n",
    "    \"\"\"Remove all diacritic marks\"\"\"\n",
    "    norm_txt = unicodedata.normalize('NFD', txt)\n",
    "    shaved = ''.join(c for c in norm_txt if not unicodedata.combining(c))\n",
    "    return unicodedata.normalize('NFC', shaved)\n",
    "\n",
    "\n",
    "order = '“Herr Voß: • ½ cup of Œtker™ caffè latte • bowl of açaí.”'\n",
    "#                                         ^                  ^\n",
    "print(shave_marks(order))\n",
    "\n",
    "Greek = 'Ζέφυρος, Zéfiro'\n",
    "#         ^        ^\n",
    "print(shave_marks(Greek))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shave_marks_latin(txt):\n",
    "    \"\"\"Remove all diacritic marks from Latin base characters\"\"\"\n",
    "    norm_txt = unicodedata.normalize('NFD', txt)\n",
    "    latin_base = False\n",
    "    keepers = []\n",
    "    for c in norm_txt:\n",
    "        if unicodedata.combining(c) and latin_base:\n",
    "            continue # ignore diacritic on Latin base char\n",
    "        keepers.append(c)\n",
    "        # if it isn't combining char, it's a new base char\n",
    "        if not unicodedata.combining(c):\n",
    "            latin_base = c in string.ascii_letters\n",
    "    shaved = ''.join(keepers)\n",
    "    return unicodedata.normalize('NFC', shaved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Herr Voß: - ½ cup of OEtker(TM) caffè latte - bowl of açaí.\"\n",
      "\"Herr Voss: - 1⁄2 cup of OEtker(TM) caffe latte - bowl of acai.\"\n"
     ]
    }
   ],
   "source": [
    "# Example 4-17. Transform some Western typographical symbols into ASCII\n",
    "\n",
    "single_map = str.maketrans(\"\"\"‚ƒ„†ˆ‹‘’“”•–—˜›\"\"\",\n",
    "                           \"\"\"'f\"*^<''\"\"---~>\"\"\")\n",
    "\n",
    "multi_map = str.maketrans({\n",
    "    '€': '<euro>',\n",
    "    '…': '...',\n",
    "    'Œ': 'OE',\n",
    "    '™': '(TM)',\n",
    "    'œ': 'oe',\n",
    "    '‰': '<per mille>',\n",
    "    '‡': '**',\n",
    "})\n",
    "\n",
    "multi_map.update(single_map)\n",
    "\n",
    "def dewinize(txt):\n",
    "    \"\"\"Replace Win1252 symbols with ASCII chars or sequences\"\"\"\n",
    "    return txt.translate(multi_map)\n",
    "\n",
    "def asciize(txt):\n",
    "    no_marks = shave_marks_latin(dewinize(txt))\n",
    "    no_marks = no_marks.replace('ß', 'ss')\n",
    "    return unicodedata.normalize('NFKC', no_marks)\n",
    "\n",
    "# Examples\n",
    "\n",
    "order = '“Herr Voß: • ½ cup of Œtker™ caffè latte • bowl of açaí.”'\n",
    "print(dewinize(order))\n",
    "print(asciize(order))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "unsupported locale setting",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-b908d9edf3b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlocale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mlocale\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetlocale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocale\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLC_COLLATE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pt_BR.UTF-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# exception will be thrown if local is not installed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mfruits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'caju'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'atemoia'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cajá'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'açaí'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'acerola'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/locale.py\u001b[0m in \u001b[0;36msetlocale\u001b[0;34m(category, locale)\u001b[0m\n\u001b[1;32m    596\u001b[0m         \u001b[0;31m# convert to string\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m         \u001b[0mlocale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_build_localename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_setlocale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mresetlocale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLC_ALL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mError\u001b[0m: unsupported locale setting"
     ]
    }
   ],
   "source": [
    "# Sorting Unicode text\n",
    "\n",
    "# Example 4-19. Using the locale.strxfrm function as sort key\n",
    "\n",
    "import locale\n",
    "\n",
    "locale.setlocale(locale.LC_COLLATE, 'pt_BR.UTF-8') # exception will be thrown if local is not installed\n",
    "\n",
    "fruits = ['caju', 'atemoia', 'cajá', 'açaí', 'acerola']\n",
    "\n",
    "sorted_fruits = sorted(fruits, key=locale.strxfrm)\n",
    "\n",
    "print(sorted_fruits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyuca'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-75f7b096c24a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Example 4-20. Using the pyuca.Collator.sort_key method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpyuca\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mcoll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyuca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCollator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mfruits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'caju'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'atemoia'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cajá'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'açaí'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'acerola'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyuca'"
     ]
    }
   ],
   "source": [
    "# Sorting with the Unicode Collation Algorithm\n",
    "# James Tauber, prolific Django contributor, must have felt the pain and created\n",
    "# PyUCA, a pure-Python implementation of the Unicode Collation Algorithm (UCA).\n",
    "# Example 4-20 shows how easy it is to use.\n",
    "# Example 4-20. Using the pyuca.Collator.sort_key method\n",
    "\n",
    "import pyuca\n",
    "coll = pyuca.Collator()\n",
    "fruits = ['caju', 'atemoia', 'cajá', 'açaí', 'acerola']\n",
    "sorted_fruits = sorted(fruits, key=coll.sort_key)\n",
    "print(sorted_fruits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U+0031\t  1   \tre_dig\tisdig\tisnum\t 1.00\tDIGIT ONE\n",
      "U+00bc\t  ¼   \t-\t-\tisnum\t 0.25\tVULGAR FRACTION ONE QUARTER\n",
      "U+00b2\t  ²   \t-\tisdig\tisnum\t 2.00\tSUPERSCRIPT TWO\n",
      "U+0969\t  ३   \tre_dig\tisdig\tisnum\t 3.00\tDEVANAGARI DIGIT THREE\n",
      "U+136b\t  ፫   \t-\tisdig\tisnum\t 3.00\tETHIOPIC DIGIT THREE\n",
      "U+216b\t  Ⅻ   \t-\t-\tisnum\t12.00\tROMAN NUMERAL TWELVE\n",
      "U+2466\t  ⑦   \t-\tisdig\tisnum\t 7.00\tCIRCLED DIGIT SEVEN\n",
      "U+2480\t  ⒀   \t-\t-\tisnum\t13.00\tPARENTHESIZED NUMBER THIRTEEN\n",
      "U+3285\t  ㊅   \t-\t-\tisnum\t 6.00\tCIRCLED IDEOGRAPH SIX\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "\n",
    "re_digit = re.compile(r'\\d')\n",
    "\n",
    "sample = '1\\xbc\\xb2\\u0969\\u136b\\u216b\\u2466\\u2480\\u3285'\n",
    "\n",
    "#http://www.unicode.org/Public/UCA/6.3.0/allkeys.txt\n",
    "\n",
    "for char in sample:\n",
    "    print('U+%04x' % ord(char),\n",
    "        char.center(6),\n",
    "        're_dig' if re_digit.match(char) else '-',\n",
    "        'isdig' if char.isdigit() else '-',\n",
    "        'isnum' if char.isnumeric() else '-',\n",
    "        format(unicodedata.numeric(char), '5.2f'),\n",
    "        unicodedata.name(char),\n",
    "        sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
